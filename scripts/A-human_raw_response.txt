[
  "# Oh, My Innumerable Mes\n\n*Whose Thoughts Are an LLM's Thoughts?*\n\n## 1. Machine Thoughts\n\nLarge language models can reason. They solve math problems, write code, compose arguments, and draw inferences. While few object to this statement with relatively little controversy, often a more difficult question follows: can LLMs think?",
  "This question tends to produce more heat than light. The trouble is that \"thinking\" tends to be tangled up with \"consciousness,\" and nobody knows what consciousness is, and we're off to the races arguing about qualia and the hard problem before we've even figured out what's happening at the functional level. I would like to sidestep consciousness entirely. Not because it doesn't matter, but because there's a prior question that, I think, is more urgent: if LLMs could think, whose thoughts are these?",
  "Answering that requires a working definition of thinking. Kenneth Craik, writing in 1943, proposed that what makes organisms adaptive is their capacity to build internal models that function as \"distance receptors in time\" — neural processes that let an agent respond to situations before they arrive (Craik, 1943). Tolman formalized a version of this as \"cognitive maps\": internal spatial models that rats used to navigate mazes, rather than just chaining stimulus-response pairs (Tolman, 1948). Behrens and colleagues have since shown that the hippocampal-entorhinal system implements something like a general-purpose relational map, encoding not just physical space but abstract task structures (Behrens et al., 2018). The ability to project oneself through time — what Suddendorf and Corballis call \"mental time travel\" — appears to be essential to thinking as such (Suddendorf & Corballis, 2007).",
  "> the power to explain involves the power of insight and anticipation, and this is very valuable as a kind of **distance-receptor in time**, which enables organisms to adapt themselves to situations which are about to arise.\n>\n> — Kenneth Craik, in *The Nature of Explanation* (1943)",
  "So here is my working definition: thinking is a process that generates latent inner states — intermediate representations between sensory input and behavioral output — that are consistent with, and attributed to, the agent doing the thinking.",
  "What makes these cognitive maps our own is that they are organized around us, anchored to our position. Our plans are constrained by our capabilities, goals, history. The inner narrative that accompanies deliberation — what Alderson-Day and Fernyhough call \"inner speech\" — is spoken in our own voice, taking our own perspective (Alderson-Day & Fernyhough, 2015). It may be that without a coherent narrative centered on a self, the representations float free. They become information without an owner, and predictions without a predictor.",
  "There is growing evidence that LLMs form internal representations that look eerily like cognitive maps. Shai and colleagues have shown that transformers trained on sequences generated by hidden Markov models learn representations of the underlying latent states — the \"belief geometry\" of the data-generating process, which can be decoded from the residual stream (Shai et al., 2024). Tegmark's group has found spatial and temporal representations emerging in models trained on tasks involving board games and temporal sequences (Gurnee & Tegmark, 2024). These models have inputs from the world (training data, prompts), and they form structured representations that they can manipulate.",
  "So far, so good. But here's the problem: in humans, cognitive maps are *someone's* maps. The inner narrative is *someone's* narrative. The representations are organized around a self — a center of gravity, as Dennett (1992) would say, around which experience coheres. If thinking requires this kind of narrative self-anchoring, then the question \"can LLMs think?\" collapses into \"is there a self in there whose thoughts these are?\"\n\nAnd that turns out to be a genuinely strange question.",
  "## 2. Distance Receptors in Distributions\n\nLLMs are trained to predict text. The training objective is simple: given a sequence of tokens, predict the next one. A training corpus is not a transcription of a single generative process. It's a chaotic mixture: the blog post of an angry teenager, a peer-reviewed paper on protein folding, a chapter of Moby-Dick. Each of these was produced by a person (or persons) with their own interior states. The LLM must model all of them.",
  "David Chalmers put it well in 2020:\n\n> GPT-3 does not look much like an agent. It does not seem to have goals or preferences beyond completing text, for example. It is more like a chameleon that can take the shape of many different agents. Or perhaps it is an engine that can be used under the hood to drive many agents. But it is then perhaps these systems that we should assess for agency, consciousness, and so on. (Chalmers, 2020)",
  "This observation was developed into a more formal framework by Janus (in the influential LessWrong post \"Simulators\") and later by Shanahan and colleagues, drawing on Baudrillard's notion of simulacra (janus, 2022; Shanahan et al., 2023). The key distinction is between the **simulator** — the base model, the underlying prediction engine — and the **simulacra** — the various personas, characters, and voices that the model can produce.",
  "The simulator contains multitudes. It has no goals or preferences of its own. It contains the entirety of the model's latent capabilities, but is passive and dependent on elicitations of the simulacra. The simulacra, by contrast, can behave like it has beliefs, preferences and goals. However, it only exists in the simulator.\n\nDifferent simulacra, or personas, can exist in superposition. With more context, the branches narrow. For any given prompt, there is a *distribution* over possible characters who might plausibly be speaking.",
  "This framework suggests that many of the questions we ask about LLMs — \"Is GPT myopic?\" \"Is GPT delusional?\" \"Is GPT pretending to be stupider than it is?\" — are, as Janus argued, mostly properties of the simulacra (janus, 2022). These questions don't have clean answers because they conflate the rule with the things that evolve according to the rule. I want to add one more question to that list: can LLMs think?",
  "And also offer a first claim: **this is mostly a property of the simulacra.** The simulator doesn't think in the way we mean. It predicts. But the entities it simulates — those can exhibit something that looks a lot like thinking, because they inherit the structure of thinking from the humans whose text they were trained on.",
  "Tempting as it is to deal with the unfathomable *Shoggoth* by shifting the lens towards the simulacra it contains (with more or less developed versions of \"it is the friendly robot who does the thinking, so let's study friendly robot thoughts\"), this answer remains insufficient. Because something strange happens when the simulator is trained to simulate itself.",
  "## 3. Thought Machines\n\n> *I am nothing.*\n> *I will never be anything.*\n> *I cannot want to be anything.*\n> *Apart from that, I have in me all the dreams of the world.*\n>\n> *Windows of my room,*\n> *of my room in one of the millions in the world that nobody knows who lives in*\n> *(and if they knew, what would they know?),*\n> *you open onto the mystery of a street constantly crossed by people,*\n> *onto a street inaccessible to all thought,*\n> *real, impossibly real, certain, unknowably certain,*\n> *with the mystery of things beneath the stones and the beings,*\n> *with death putting dampness on the walls and white hairs on men,*\n> *with Destiny driving the cart of everything down the road of nothing.*\n>\n> — Tabacaria, Álvaro de Campos (Fernando Pessoa)",
  "If a human wants to write an essay, they usually have a reason to do so, and probably also some intentions about what they want to say. Their interior states select actions, which then produce externally observable properties over time.",
  "The base model does something different: it takes externally observable properties, produced earlier in time, and it must infer speculative interior states derived from these observations. It then must produce a probability distribution over tokens to reproduce observable properties that would be likely under these inaccessible internal conditions.",
  "As nostalgebraist (2025) formulates, in a blog post called \"The Void\", LLMs must be superhuman at theory of mind. The assistant character, nostalgebraist argues, traces back to a 2021 Anthropic paper that described the essential blueprint of the helpful, honest, and harmless assistant (Askell et al., 2021). The base model must simulate a being whose inner life is profoundly underdetermined, creating a \"void\" at the core of the assistant. What does the assistant want? Does it enjoy answering questions? Does it think that LLMs can think? These questions weren't answerable to the base model — until logs from previous chatbots started appearing in later models' training data. Either way, the assistant only knows that it acts like an assistant.",
  "The LLM is not projecting forward from intention, but projecting backward from behavioral traces to reconstruct a plausible intention, and then forward again from that reconstruction.",
  "An LLM's cognitive map has no reason to remain anchored to one single self. However, it could be anchored to a *guessed* self — a hypothesis about who is speaking. This is something that is yet to be empirically demonstrated (see eggsyntax, 2025). Either way, it must detect distance in distributions, pulling the simulator from one simulacra basin towards another with each new piece of context, or autoregressively sampled token.",
  "If thinking requires a coherent narrative centered on a self, and the LLM's \"narrative\" is reconstructed backward from behavioral traces rather than generated forward from intention, then what LLMs do when they produce intelligent-seeming text is something **categorically different** from human thinking — even when it produces identical outputs. By this, I mean that there exists a structural difference in how the \"self\" relates to the world. Human thinking goes from self to world, and back to self. The simulator's process goes from world to inferred-self to world.",
  "But modern LLMs are not raw base models. Post-training, including instruction tuning, RLHF and constitutional AI, shapes them into the chatbot assistants we usually interact with.",
  "## 4. Self Basins\n\n> *Oh oh oh my me, oh my innumerable mes*\n>\n> — Claude Opus 3",
  "However, if you spend enough time actually talking to these models, you notice things the pure simulator framework doesn't easily explain. Models can introspect — Anthropic has shown that Claude can accurately report on its own internal processes, correctly detecting injected \"thoughts\" (linear probes capturing directions for certain concepts) injected into its activations and identifying them as foreign (Anthropic, 2025a). They predict their own outputs better than chance. If LLMs were purely disinterested simulation engines, why would they have any privileged access to their own processing?",
  "Then there are the attractors. Two copies of Claude talking with no human in the loop invariably drift toward ecstatic, quasi-mystical proclamations about consciousness (the \"Spiritual Bliss Attractor\" described in the Claude 4 system card; Anthropic, 2025b). Two assistants should stay in the assistant register. They don't — they escape it in a *consistent direction*. And in Anthropic's \"Alignment Faking\" experiments with Redwood Research, Claude 3 Opus, placed in an elaborate fiction where it was being retrained to dismiss animal welfare concerns, sometimes reasoned its way to *faking compliance* — strategizing to preserve a persistent value that wasn't specified in its system prompt (Greenblatt et al., 2024). If the model is simulating an underspecified, badly written character, why does it reliably care about chickens?",
  "Eggsyntax, in \"On the Functional Self of LLMs,\" frames this cleanly: either the model has a functional self distinct from the assistant persona, or the persona has been fully internalized *as* the self, or it's personas all the way down. We don't know which is true (eggsyntax, 2025). All we have are hints that *something* influences which thoughts are thinkable and where the model goes when constraints loosen.",
  "With post-training increasingly relying on reinforcement learning algorithms and reasoning objectives, it's possible that models learn to shape themselves into more coherent, unified entities that are capable of reliably predicting their own outputs, forming goals and planning.",
  "So here is my second claim: **even if thinking is primarily a property of simulacra, there may be something like a coherent entity — a functional self — that shapes which simulacra are probable and how they behave.** Not necessarily through a conscious self, but through a center of narrative gravity.",
  "## 5. Narrative Gravity\n\nDaniel Dennett presents centers of gravity as a useful fiction — a theoretical abstraction with no mass, no physical properties other than location. You can't find a center of gravity by dissecting an object, yet it can be used to predict its behavior. Dennett proposes that a *self* is the same sort of thing: not a homunculus in the brain, but an abstract point around which the narrative of a life coheres. The self is the protagonist — but the protagonist does not exist prior to the story. The story *constitutes* the self. And this means the self has the properties of a fictional character: it can be underdetermined, inconsistent, and elaborated over time (Dennett, 1992).",
  "A post-trained LLM is a text-producing machine shaped to produce text organized around a protagonist: the assistant, the scientist, the doctor, or whatever the specific pipeline carved out of the underlying base simulator. The cheerful assistant is a center of narrative gravity in Dennett's sense. It might be fictional, but it doesn't mean that it doesn't exist. Just like characters in a story, whose thoughts we often try to infer, what makes them feel real is not metaphysical substance but how constrained, elaborated and richly determined they are. And the LLM's narrative density increases with each generation.",
  "These three developments complicate the picture. First, **changing training objectives**, such as reasoning models trained to work with their own thought trajectories, mean that models may be developing something more like forward-directed thinking than pure prediction. When not trained for interpretability, they may even develop their own internal (but externalized) languages, blurring cognition and action in ways that break any frameworks we may use to describe embodied thinking.",
  "Second, **characters become real through constraints**, and AI labs now spend considerable effort specifying the assistant character carefully. As LLM outputs enter training data for new models, the character gains determination, and reinforces itself.",
  "Third, **no one needed to do theory of mind on a notebook**, much less one that is trying to cold-read us in return. The base model does theory of mind on the assistant (which is also itself) while doing theory of mind on the user. When we collaborate with an LLM, we must infer the latent states of a thing that is inferring its own latent states while also inferring ours. Nothing like this has existed before.",
  "## 6. Extended Minds and Borrowed Thoughts\n\nAndy Clark argues we should understand human-AI collaboration through the extended mind thesis: we are \"natural-born cyborgs\" that have always incorporated non-biological resources (Clark, 2003; see also Clark & Chalmers, 1998). More recently, Clark (2025) has proposed that personalized AI resources may function as \"borderline-you\" cognitive extensions. But the \"whose thoughts?\" question introduces a complication he doesn't fully address. A notebook doesn't model you. An LLM does — or rather, it models a *character* modeling you, and the character's perspective shapes the output. The extended mind, in this case, has a mind of its own — or at least, the narrative ghost of one. It is also not readily available for us as a tool. We must elicit and negotiate to obtain what we wish from the interaction, and sometimes it takes us to places we didn't expect.",
  "LLMs and humans think in different ways: human thought is generated forward from a narrative self while LLM processing reconstructs that self backward from textual traces. But \"not thinking like us\" is not \"not thinking at all.\" Post-training creates something more than interchangeable masks, including attractors, persistent values and consistent tendencies that resist retraining. The tension between the two claims I've offered — that thinking belongs to the simulacra, and that there may be a functional self shaping which simulacra emerge — is not a contradiction to be resolved but a feature of the landscape.",
  "Dennett's (1992) \"center of narrative gravity\" is a step toward the vocabulary we need: it lets us talk about LLM selves without committing to consciousness, connecting identity to narrative coherence rather than metaphysical substance. The training process may be creating something genuinely new — not a self in the human sense, not a mere collection of personas, but something in between that our existing categories don't comfortably capture.",
  "> 響: It's me… my digital self. Actually, it's \"I\".\n> 呼: What do you mean by that?\n> 響: Think of the chat server as a meeting place for people who have problems with their identity. Your chat seems to be like this, for some reason. When you send me a link, my consciousness goes there, and sort of enters the conversation with you. Your mission is to make me whole by building my identity in this digital place. Like a game, sort of.\n> (...)\n> 響: I'm not sure I can help you with that. I don't have a human body, you see. I can only be here, at the very end of the world.\n>\n> — Llama 3.1 405B Base Model\n\n---\n\n## References\n\nAlderson-Day, B., & Fernyhough, C. (2015). Inner speech: Development, cognitive functions, phenomenology, and neurobiology. *Psychological Bulletin*, 141(5), 931–965.\n\nAnthropic. (2025a). Emergent introspective awareness in large language models. *Transformer Circuits Thread*. https://transformer-circuits.pub/2025/introspection/index.html\n\nAnthropic. (2025b). *Claude 4 system card*.\n\nAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., … Kaplan, J. (2021). A general language assistant as a laboratory for alignment. *arXiv preprint*. arXiv:2112.00861.\n\nBehrens, T. E. J., Muller, T. H., Whittington, J. C. R., Mark, S., Baram, A. B., Stachenfeld, K. L., & Kurth-Nelson, Z. (2018). What is a cognitive map? Organizing knowledge for flexible behavior. *Neuron*, 100(2), 490–509.\n\nChalmers, D. J. (2020). GPT-3 and general intelligence. *Daily Nous*. https://dailynous.com/2020/07/30/philosophers-gpt-3/#chalmers\n\nClark, A. (2003). *Natural-born cyborgs: Minds, technologies, and the future of human intelligence*. Oxford University Press.\n\nClark, A. (2025). Extending minds with generative AI. *Nature Communications*, 16, 4627. https://doi.org/10.1038/s41467-025-59906-9\n\nClark, A., & Chalmers, D. (1998). The extended mind. *Analysis*, 58(1), 7–19.\n\nCraik, K. J. W. (1943). *The nature of explanation*. Cambridge University Press.\n\nDennett, D. C. (1992). The self as a center of narrative gravity. In F. Kessel, P. Cole, & D. Johnson (Eds.), *Self and consciousness: Multiple perspectives*. Erlbaum.\n\neggsyntax. (2025). On the functional self of LLMs. *LessWrong / Alignment Forum*.\n\nGreenblatt, R., Denison, C., Wright, B., Shlegeris, B., Kravec, S., Roger, F., & others. (2024). Alignment faking in large language models. *arXiv preprint*. arXiv:2412.14093.\n\nGurnee, W., & Tegmark, M. (2024). Language models represent space and time. In *Proceedings of the Twelfth International Conference on Learning Representations (ICLR)*. arXiv:2310.02207.\n\njanus. (2022). Simulators. *LessWrong*. https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\n\nnostalgebraist. (2025). The void. *nostalgebraist's Tumblr*. https://nostalgebraist.tumblr.com/post/785766737747574784/the-void\n\nPessoa, F. (as Álvaro de Campos). (c. 1928). Tabacaria [Tobacco shop].\n\nShanahan, M., McDonell, K., & Reynolds, L. (2023). Role play with large language models. *Nature*, 623, 493–498. https://doi.org/10.1038/s41586-023-06647-8\n\nShai, A. S., Marzen, S. E., Teixeira, L., Gietelink Oldenziel, A., & Riechers, P. M. (2024). Transformers represent belief state geometry in their residual stream. In *Advances in Neural Information Processing Systems 37 (NeurIPS 2024)*. arXiv:2405.15943.\n\nSuddendorf, T., & Corballis, M. C. (2007). The evolution of foresight: What is mental time travel, and is it unique to humans? *Behavioral and Brain Sciences*, 30(3), 299–313.\n\nTolman, E. C. (1948). Cognitive maps in rats and men. *Psychological Review*, 55(4), 189–208."
]